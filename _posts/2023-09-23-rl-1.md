# A journey into Reinforcement Learning (With Minimal Math)

Reinforcement Learning (RL) is a term that often surfaces in discussions surrounding Artificial Intelligence (AI) and Machine Learning (ML). Known for its robust applications spanning robotics, finance, and autonomous driving, RL stands as a potent field in the technological realm. However, delving into Reinforcement Learning typically demands a solid mathematical base, as its concepts and algorithms frequently take the form of intricate mathematical expressions. This mathematical complexity can pose a barrier, even for those with a technical background.

Last year, I found myself in the same boat when I embarked on a journey through Stanford CS234: Reinforcement Learning. Within the first few hours of study, my screen seemed to be filled with mathematical equations I hadn't seen since Calc 2 in college, nearly a decade ago. It felt like diving headfirst into an abyss of complexity. However, the underlying concepts of RL are quite intuitive and it is possible to develop an decent understanding of the field without delving into the complex mathematics.

This blog is my attempt to demystify Reinforcement Learning, offering explanations of its core concepts in an accessible manner without the overwhelming mathematical jargon. My aim is to break down RL's fundamentals into simpler terms, making it understandable for those who may have a background and expertise in AI and Computer Science, but not in the deep mathematics required in RL.


## What is Reinforcement Learning?
Reinforcement Learning, at its core, involves training an AI bot to make decisions by taking actions and then learning from the results of those actions, similar to how animals learn through trial and error. Imagine training a dog to do tricks. You reward the dog with a treat when it does the right thing, and you don't reward it when it does the wrong thing. Over time, the dog learns which actions lead to getting a treat and which don't.

![](/images/rl/rl-1.png "Reinforcement Learning")


This same approach is used to train AI programs. The goal of Reinforcement Learning is to teach the AI a strategy that allows it to make decisions leading to favorable outcomes (like a robot learning to walk or an autonomous car driving safely to its destination) even in uncertain situations or environments.

## How to define a Reinforcement Learning problem?

In reinforcement learning problems, an agent interacts with an environment through a series of states and actions. At each state, the agent chooses an action, which leads to a new state and a reward. The goal is to learn a policy that maximizes the cumulative reward over time. This learning process often involves building models of the environment's dynamics, though some approaches are model-free. The agent's ability to evaluate the long-term consequences of actions, guided by rewards, is fundamental to mastering the problem.




Let’s try to understand the terminology used above using the example of training an AI to play Tic-Tac-Toe.

![](/images/rl/rl-tic-1.png "Tic-Tac-Toe")


**Agent**: An agent is the computer program that we are trying to train.

**Environment**: The environment is the representation of the world in which the agent operates. In a Tic-Tac-Toe game, the environment is the 3×3 board with 9 possible positions.

**State**: A state represents the current configuration of the board. There are 9 squares on the board and each square can be filled with either an X, O, or blank. Therefore simplistically, the total number of states is 3⁹ = 19,683 states. (P.S. calculating the actual number of states in a Tic-Tac-Toe game is a subject of research from a number of people over the years [1][2])

**Action**: An action is the decision the agent takes about which of the 9 squares to place its X. The number of possible actions at the beginning of the game is 9 and the number goes down with every turn.

Reward (R): A reward is the “feedback” the agent receives for taking a particular action. Rewards are at the heart of reinforcement learning and setting up the reward structure properly is the key to getting the agent to perform well. In a game of Tic-Tac-Toe, we could set up the reward structure as follows:

- Reward for Winning (+10 points)
- Reward for Blocking Opponent from Winning (+5 points)
- Reward for Intermediate Progress (+1 point)
- Reward for Neutral Move (0 points)
- Reward for Draw (0 points)
- Penalty for Losing (-10 points)

The agent’s goal is to gain the maximum reward over the course of the game, so by setting up the reward structure we incentivize or disincentivize the agent from taking certain actions.

**Terminal State**: A terminal state is a state where the game ends. In Tic-Tac-Toe, the game ends with either win for one of the players or when the board is filled.

**Policy**: A policy is the strategy that the agent will employ to choose the next action. In an RL system, the agent will “query” the policy for the best action in the current state. For Tic Tac Toe, the policy might look something like this:

State: If there's an opportunity to win the game on your next move (e.g., you have two of your symbols in a row), then:
    Action: Place your symbol in the winning position to win the game.

State: If the opponent is about to win on their next move (e.g., they have two of their symbols in a row), then:
    Action: Block the opponent's winning move by placing your symbol in the blocking position.

State: If neither you nor the opponent can win on the next move, and you have the center square available:
    Action: Place your symbol in the center square.

State: If none of the above conditions apply, and you don't have an immediate winning move or a blocking move:
    Action: Place your symbol in any available empty square randomly.

This is, of course, an incomplete set of State-Action pairs that would be contained in a policy for a Tic-Tac-Toe game. The total number of State-Action pairs required to fully model the game would be # of possible states x # of possible actions = 19683 × 9 = 177,147 possible pairs.

This number, while large, is not intractable for modern computers. Therefore, it is possible to train an RL agent to play a game of Tic-Tac-Toe perfectly in every possible situation (Optimal Policy). In more complex RL scenarios such as robotics or autonomous driving, the optimal policy is not easy to identify (often impossible).

**Expected sum of future rewards**: This refers to the concept of assigning a value to each state and action pair based on the expected future rewards that can be obtained by following a certain policy. It involves considering not only the immediate reward received from an action but also the potential rewards that may come in the future. The expected sum of future rewards incentivizes the agent to prioritize the long term goal, not just focus on immediate rewards.

Let’s try to understand this intuitively. Imagine this situation in a Tic-Tac-Toe game:

![](/images/rl/rl-tic-2.png "Tic-Tac-Toe 2: O to play")


The Agent (O) is about to make a move. There are a few things to consider -

The winning options for the agent are 1-5-9 or 2-5-8. 

From the current state, it seems that the opponent (X) is making a play for 1-4-7

So the agent can play 1,2,8,9 to make progress towards a winning move. Or the agent can play 1 or 7 to block the opponent.

Based on the reward structure we laid out above, let’s calculate the total future reward the agent will get in each of these cases:

Case 2,8,9: The agent would get 1 point for making an intermediate move. However, the intermediate move is only useful because it progresses the agent towards a victory. Therefore, if we sum up all the rewards the agent expects to get from this state to the end of game, we get 1 (current move) + 10 (future winning move) = 11 points.

Case 7: The agent would get 5 points for blocking the opponent. But that does not open a path for the agent to go on to win the game. Therefore, if the agent takes this path, the total rewards the agent would expect to get would be 5 points.

Case 1: In this case, the agent is doing two things at once. The agent is blocking the opponent AND advancing the game towards a victory. Therefore, the sum of expected rewards from this move would be:

5 points (blocking the opponent) + 1 point (intermediate move) + 10 points (future victory) = 16 points.

Therefore, the optimal action in this case would be to play Square 1. By considering the total sum of rewards, the agent was able to identify the optimal action accurately. It is important to note that this expected reward is only relevant to the current state. When the opponent makes their next move, the expected sum of future rewards will be updated, thus leading to a different optimal action.

**Discount Factor**: A discount factor is a concept in reinforcement learning (and in Finance) that denotes how much a person values future reward vs current reward. For example, if I offer you the choice between $100 today or $120 a year from now, you will most likely choose the $100 today. However, if the choice is between $100 today and $10,000 a year from now, the choice is harder - probably depends on how much money you have today and how much you need the $100.Applying this principle to Reinforcement Learning, the discount factor determines how much the agent will value future rewards vs current rewards. It is usually represented by a numerical value between 0 and 1.

- High Discount Factor (Close to 1): When the discount factor is close to 1, the agent values future rewards almost as much as immediate rewards. It prioritizes long-term objectives and is willing to make sacrifices in the short term for the potential of higher cumulative rewards over time. This setting is suitable when the agent has a patient and forward-looking strategy.

- Low Discount Factor (Close to 0): When the discount factor is close to 0, the agent heavily discounts future rewards and places a strong emphasis on immediate rewards. It becomes more focused on short-term gains and is less concerned with long-term objectives. This setting is suitable when the agent has a myopic or impatient strategy.

**Value Function**: A value function combines the concept of expected sum of future rewards and discount factor. In simple terms, the value function calculates the total estimated future discounted reward that would be earned by visiting each state in the future when following a policy until reaching the terminating state. The higher the total future discounted reward, the “better” the policy is. The value function of the optimal policy is called optimal value function and it represents the best possible reward an agent can get in the given environment.

**Markov Decision Processes**: A Markov Decision Process (MDP) is a mathematical framework that is used to represent Reinforcement Learning problems. The simplest way of thinking about an MDP is as a flow chart of states and actions. For our Tic-Tac-Toe AI, a simple flowchart would look like this:

![](/images/rl/rl-tic-3.png "Simplified MDP for Tic-Tac-Toe")

As seen in the flowchart above, an MDP represents all the states, actions and corresponding rewards in a tic-tac-toe game. This flowchart is a small representation of the 177,147 possible state action pairs.Mathematically, an MDP is represented as a tuple with 5 elements => M = <S,A,P,R,γ> where,S = set of statesA = set of actionsP = transition probability functionR = Reward functionγ = Discount Factor

By using the terminology we defined above, most (if not all) reinforcement learning problems can be expressed. 

---

## Complexity of Real World RL problems

In the above section we learned about the building blocks of a reinforcement learning problem using the game of Tic Tac Toe. However in reality, tic-tac-toe is an incredibly simple game with small number of total states, limited, well defined actions, and clear rewards. Developing an optimal policy for such a problem is an easy task. 

The complexity of real world reinforcement learning systems can scale considerably and in multiple dimensions, making the task of identifying or developing an optimal policy very very hard (or often impossible). Let’s look at some of the dimensions that make real world systems complex.

**Number of States**: In our tic-tac-toe example, we calculated 19,683 possible states the game can be in. But let’s take a look at some other popular  games:

- Backgammon: 10^20 possible board combinations. For perspective, there are 10^19 grains of sand on all the beaches on Earth.

- Chess: 10^43 possible board combinations. This number is on the scale of the number of atoms that make up the Earth.

- Go: 10^170 possible board combinations. This number is so mind bogglingly large that there is no comparison that can be used to accurately explain the scale. For reference, the number of atoms in the observable universe is 10^80, which is 10^90 times smaller than the number of possible board combinations in Go.

**Continuous State**: Imagine a robotic arm in a manufacturing plant that that needs to move with high precision. The arm's movement can vary along a continuum in terms of direction and extent. Such scenarios are called “Continuous State”, in that the number of possible states in the environment is infinite. Continuous states are considerably more difficult to model, as it is impossible to have a distinct value or policy for every single state.

**Unpredictability (Stochasticity) of Environment**: Stochasticity refers to the presence of randomness or unpredictability in the outcomes of actions in the environment. In a stochastic environment, performing the same action under the same conditions can lead to different outcomes each time. In a card game like poker, the environment is unpredictable. The same action (e.g., betting a certain amount) can have different outcomes depending on the randomness of the dealt cards.

**Observability**: In many real-world scenarios, the agent doesn't have access to the full state of the environment. This situation, known as a Partially Observable Markov Decision Process (POMDP), requires the agent to make decisions based on incomplete or uncertain information, which is significantly more complex than fully observable environments. An example of a partially observable environment is the stock market. Trading bots have limited market information and are often dealing with volatility in stock price due to actions of other traders that are not visible to it.

**Dynamic Environment**: Imagine a self-driving car that is driving in traffic on a city road. Its environment (and hence state) is constantly evolving as traffic moves and as it encounters road signs, pedestrians, etc. These changes are not solely the result of the vehicle's actions but are influenced by a multitude of external factors. The RL agent must be capable of adjusting its strategy as the environment evolves, which requires the agent to be flexible and able to learn continuously from new experiences.

Apart from these, real world systems sometimes have high dimensional action spaces (large number of possible actions at every state), long term dependencies (an action taken in the present may not pay off till far into the future. E.g., chess move), or there could be multi-agent systems where a number of RL agents act in unison to achieve a result (complex multi-player games such as Dota or Counter Strike).

Understanding the environment, the state space, and the action space for your specific problem is critical to effectively evaluating what reinforcement learning algorithm is needed. In the next post, we will look at reinforcement learning algorithms and understand how they work.

Here's the table of contents:

1. TOC
{:toc}

## Basic setup




Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-filename.md`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `filename` is whatever file name you choose, to remind yourself what this post is about. `.md` is the file extension for markdown files.

The first line of the file should start with a single hash character, then a space, then your title. This is how you create a "*level 1 heading*" in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line `## File names` above.

## Basic formatting

You can use *italics*, **bold**, `code font text`, and create [links](https://www.markdownguide.org/cheat-sheet/). Here's a footnote [^1]. Here's a horizontal rule:

---

## Lists

Here's a list:

- item 1
- item 2

And a numbered list:

1. item 1
1. item 2

## Boxes and stuff

> This is a quotation

{% include alert.html text="You can include alert boxes" %}

...and...

{% include info.html text="You can include info boxes" %}

## Images

![](/images/logo.png "fast.ai's logo")

## Code

General preformatted text:

    # Do a thing
    do_thing()

Python code and output:

```python
# Prints '2'
print(1+1)
```

    2

## Tables

| Column 1 | Column 2 |
|-|-|
| A thing | Another thing |

## Footnotes

[^1]: This is the footnote.

