# CPUs vs GPUs: What I learned building my AI rig

Deep Learning is an intense computational process. After spending a lot of money on Azure and AWS VM instances for training deep learning models, I finally bit the bullet and decided to build a deep learning computer at home. This turned out to be a fun and rewarding experience that taught me a lot about how AI hardware actually works, why GPUs matter so much in training neural networks, and how to make the right tradeoffs while building a rig that's right for your use cases.
This post is my effort at explaining what I've learned.

## Experience building the rig

Back in April 2023, Elon Musk tweeted "Everyone and their dog is buying GPUs". Indeed if you go around looking for GPUs that have enough juice to train LLMs or other forms of generative models, you would be shocked at the premium you would have to pay. For example, an NVidia H100 GPU that retails at $25,000-$30,000 would set you back nearly $50,000 (with a 2 month lead time). On the enthusiast side of the market, things are even worse. An NVidia 4090, which is the best consumer GPU right now, retails at $1599 but has not been seen at that price since launch day.
So when I set out to build my AI rig, I realized I was looking at plunking down thousands of dollars of markup to scalpers if I wanted the top-of-the-line GPU. No can do.

