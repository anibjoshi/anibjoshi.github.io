# A journey into Reinforcement Learning (With Minimal Math) - Part 2


In the previous post, we laid out the framework through which reinforcement learning problems are defined. This is crucial, as the naure of the environment, the number of states, the number of actions and the predictability of the system determines which RL algorithm is needed to solve the problem. In this post, we will dig into reinforcement learning algorithms and understand how a solution to the problem is identified using the algorithms.

## Recap: What is the reinforcement learning algorithm trying to do?

As we learned in the earlier post, a reinforcement learning agent is operating in an environment which has a number of states. The agent can take actions which provides it with positive or negative reward. The agent’s purpose is to maximize its expected cumulative reward In order to maximize its cumulative reward, the agent has to develop its strategy and iteratively change the strategy as it takes action and gets feedback from the environment. 
This “strategy” is called a Policy. 

So in short, a RL agent is trying to identify the best policy to take actions in a given environment that will maximize its cumulative reward. Where does the RL algorithm come into the picture? In simple terms, the algorithm is what the agent uses to identify the best policy that will maximize its cumulative reward.

## Exploration vs Exploitation

When the agent is first activated in a new environment, it has no reliable mechanism to identify the most optimal next action. Therefore, the agent first spends some cycles trying out a variety of actions and getting the associated reward. Each cycle gives the agent information about the nature of the environment, which in turn helps it finetune its future strategy. After a sufficient number of "random" actions, the agent decides to start acting on the intelligence it has gathered. After a few cycles of "explotiing" the gained intelligence and making progress, the agent could run into previously unknown states and rewards. This would neccessitate the agent to go back into "exloration" mode to gain more intelligence.

This is the exploration vs exploitation dilemma is one of the fundamental concepts in reinforcement learning.

**Exploration**: This is where the agent tries new actions and explores the environment to gather new information. It's about venturing into the unknown to find out how different actions affect outcomes. Exploration is crucial, especially in the early stages of learning or in dynamic environments, as it allows the agent to build a knowledge base about the effects of its actions.

**Exploitation**: This is when the agent uses the knowledge it has already acquired to make decisions that maximize its reward. The agent exploits its current understanding of the environment to make the best possible choices.

The balance between exploration and exploitation is an important challenge while training reinforcement learning agents. Too much exploration can lead to inefficiency, as the agent might keep trying suboptimal actions without capitalizing on what it has learned. On the other hand, too much exploitation can cause the agent to get stuck in a local optimum without discovering potentially better options.

Different RL algorithms handle the exploration-exploitation trade-off in various ways (ε-greedy, Softmax, Upper Confidence Bound, Thompson Sampling, etc.)

## Convergence

Over time as the agent navigates the environment, alternating between exploration and exploitation over numerous rounds, it arrives at a stage where subsequent actions no longer significantly alter the policy needed to attain the maximum reward. This state, where the policy becomes "stable," is known as convergence. Convergence is the process where a reinforcement learning (RL) agent gradually moves towards a steady solution or policy. Ideally, this stable policy is the best possible strategy for the specific task or environment the agent is interacting with.

When an agent achieves convergence, its training is assumed to be completed and the agent can now use the learned policy to navigate the environment till it reaches the terminal state.

In real-world applications, perfect convergence is often challenging. Instead, practitioners look for practical convergence where the policy is good enough for the task at hand, even if it's not mathematically the optimal policy. Additionally, in non-stationary environments or those with very large state spaces, convergence can be a moving target, requiring continuous learning and adaptation.

## An overview of the universe of reinforcement learning algorithms:

Now that we have understood what RL algorithms are doing, we’ll broadly go over the universe of reinforcement learning algorithms and try to understand how reinforcement learning algorithms are classified. I will refrain from digging into the technical specifics of any algorithms, but I have referenced resources at the end that do an excellent job of explaining the technical details.

![](/images/rl/rl-tic-4.png "Source: OpenAI Spinning Up")

There are a number of ways to classify reinforcement learning algorithms, but I have used the classification that OpenAI used in its Spinning Up program

**Model-Free vs Model Based**: 
Model-Free RL is a method where the agent learns decision-making and actions directly from interacting with the environment, without using a model of its dynamics. It relies on trial and error to learn a policy for actions in each state.
Model-Based RL is a method where the agent either learns or utilizes a model representing the environment's behavior and dynamics. This model is used to predict responses to its actions, enabling the agent to simulate and plan actions effectively.
In simple terms, Model-Free RL is like learning to play a game by playing it over and over, learning from successes and mistakes, while Model-Based RL is like having a walkthrough or guide that explains how the game works, allowing you to think ahead and plan strategies before playing.

**Policy Based Methods vs Value Based Methods**:Model Free methods can be further classified into policy based methods and value based methods. Value Based methods focus on directly learning the value function. The policy is derived indirectly from the value function. Q-Learning and SARSA are common value-based methods.Policy-based methods directly learn the policy and try to identify the optimal policy for the problem. Policy Gradient (REINFORCE) and Actor-Critic algorithms fall under this category.

**On-Policy vs. Off-Policy Learning**:On-Policy Learning refers to the case when the RL algorithm gets the value of the policy that is currently being used to make decisions. The RL algorithm uses the same policy to derive actions that it is updating and improving.Off-Policy Learning refers to the case when the RL algorithm follows one policy and updates a different policy. The policy that the algorithm uses to derive actions is different from the one it is updating and improving.


## Introduction to some popular RL algorithms

While the technical details and implementation of RL algorithms is outside the scope of this post, it would be useful to look at a few well known classical RL algorithms to build intuition. We will also apply the algorithm to the Tic-Tac-Toe example we looked at in the earlier post.

**Dynamic Programming:** Dynamic Programming (DP) is used in RL when we have a complete and accurate model of the environment. It involves breaking down a complex problem into simpler subproblems and solving them systematically. In RL, DP is used to find the best policy (set of actions) to maximize rewards in a given environment.

![](/images/rl/rl-tic-5.png "Source: David Silver")

**Tic-Tac-Toe:** An agent using DP would first analyze all possible end states of the game (win, lose, draw). It would then systematically work backwards, examining every possible board configuration. For each configuration, the agent calculates the best possible move, considering all potential future moves by the opponent. DP requires the agent to compute and store the outcomes of every possible move for every board state, a task that is computationally intensive but feasible for a simple game like Tic Tac Toe.

**Monte Carlo Methods:** Monte Carlo Methods learn directly from complete episodes of experience, without a model of the environment. They update the value of states based on the average of the returns (rewards) observed after visiting those states.

![](/images/rl/rl-tic-6.png "Source: David Silver")

**Tic-Tac-Toe:** An agent using Monte Carlo Methods would play many games of Tic Tac Toe to completion. After each game, it updates its strategy based on whether it won, lost, or drew. For instance, if the agent observes that it often wins by placing 'O's in the center, it averages out these winning outcomes. The agent learns effective strategies based on the results of entire games, not on individual moves.


**Temporal Difference Learning:** Temporal Difference (TD) Learning is a blend of Monte Carlo and Dynamic Programming methods. It updates the value estimates based on the difference (temporal difference) between consecutive predictions, without needing to wait until the end of the episode.

![](/images/rl/rl-tic-7.png "Source: David Silver")

**Tic-Tac-Toe:** Suppose the agent is partway through a game and predicts a high chance of winning. After making a move, the game state changes, perhaps reducing its chances of winning. The agent using TD Learning immediately adjusts its predictions based on this new information, not waiting for the game's end. If placing an 'O' in a corner seemed initially advantageous but leads to a disadvantageous position against a skilled opponent, the AI quickly updates its strategy, learning from the immediate outcome of each move.

**SARSA (State-Action-Reward-State-Action):** SARSA is an on-policy method that updates the value of a state-action pair based on the states and actions it actually takes, including the next action (hence the name SARSA). It learns a policy and follows it at the same time.

![](/images/rl/rl-tic-8.png "Source: Lillian Weng")

**Tic-Tac-Toe:** An agent using SARSA makes a move and then immediately considers the outcome of this move in conjunction with the next planned move. For example, if it places an 'O' in a corner and plans the next move based on the opponent's response, it updates its strategy by considering this sequence of actions. The agent learns a strategy that accounts for the consequences of its current action and its next planned action in the game.

**Q-Learning:** Q-Learning is an off-policy method that learns the value of the best action to take in a given state (Q-values). It updates its estimates based not on the action it takes, but on the best possible action it could have taken.

![](/images/rl/rl-tic-09.png "Simplified Representation of Q-Learning")

**Tic-Tac-Toe:** An agent using Q-Learning would have a Q-table where each state of the game board is a row, and each possible action (placing an 'O' in one of the empty spots) is a column. Initially, the Q-values in the table might be random or set to a default value.As the agent plays the game, it updates the Q-values based on the outcomes. For example, if the agent places an 'O' in the center and this move leads to a win in several games, the Q-value for this state-action pair (state of the board before the move, and the action of placing an 'O' in the center) would be increased. Conversely, if a certain move leads consistently to a loss, the corresponding Q-value would decrease.The agent learns the best moves (actions) for each board configuration (state) by continuously updating this Q-table based on the rewards (wins, losses, or draws) it experiences. Over time, the Q-table will guide the agent to make the most optimal moves in different board states, having learned from its successes and failures in previous games.


## Deep Reinforcement Learning: Combining Reinforcement Learning with Deep Learning

In 2013, DeepMind released a landmark paper titled “Playing Atari with Deep Reinforcement Learning” in which the researchers built an RL agent that could play classic Atari games such as Pong, Breakout, and Space Invaders solely using screen images of the game as input. This was a remarkable leap in reinforcement learning capabilities and artificial intelligence in general, because it closely maps how humans learn to play video games. 

Deep Reinforcement Learning has become a major area of research in AI and has led to impressive new breakthroughs in the field. Let’s try to understand what Deep RL is, why is it needed, and how it performs so well.

**Limitations of Classical Reinforcement Learning**

Classical Reinforcement Learning (RL) techniques, though effective in certain scenarios, face significant challenges when applied to complex systems and environments. One of the main limitations of classical RL methods is their inability to process intricate input types. These traditional techniques are designed to handle simple, numerical inputs, but struggle with more complex forms such as images, videos, audio, and text. This limitation restricts their applicability in diverse and visually rich environments.

Moreover, the performance of classical RL algorithms, like Q-Learning, deteriorates as the size of the state and action spaces increases. To understand this better, consider the game of Tic-Tac-Toe. This game, with its approximately 180,000 possible state-action pairs, can be effectively managed using Q-Learning. However, when we scale up to a game like Chess, which has an overwhelming number of possible state-action pairs (around 10^43), Q-Learning falls short. For such a vast game, constructing and utilizing a complete Q-table is impractical, both in terms of memory space and computational time.

The crux of the problem lies in the sheer complexity and volume of data. In cases like Chess, where there are a tremendous number of potential moves and outcomes, classical RL algorithms cannot efficiently map every possible scenario.

To address this limitation, what's needed is a technique that can function as an effective approximator – one that can predict the most accurate outcomes without explicitly mapping every possible state-action pair. Luckily, we have a technique that is built exactly for that: Neural Networks.

**Deep Learning**

Deep Learning refers to the field of AI that mimics the workings of the human brain in processing data and creating patterns for use in decision making. It's built upon neural networks, which are algorithms modeled after the human brain.

Let’s say we want to teach a child to identify apples. We would start by showing a few apples and ask them to point out some characteristics they can identify. The child would be able to identify characteristics such as:

- Color: Apples are red, green, or yellow.
- Shape: Apples are round with a slight dip at the top and bottom with a stem at the top.
- Texture: Apples have a smooth surface.

With enough training, the child will be able to look at a fruit in the supermarket and identify whether it is an apple (or something else).

The same approach can be used to train a deep learning model to identify apples. We show a few thousand images of apples to the model. The model takes in raw pixels from the images and identify characteristics or “features” from the image. For simplicity, we can assume that each neuron in the model identifies one distinct characteristic. After seeing enough images, the model can look at a new image and identify with reasonable accuracy whether it has an apple in it.

![](/images/rl/rl-tic-10.png "Simplfied Representation of the deep learning process")

Neural networks are highly generalizable and can perform well at very large feature sizes and input sizes. They are also excellent at processing high-dimensional data (like pixels in an image), which allows them to operate in more complex and visually rich environments. Finally, as neural networks automatically detect and create relevant features from raw data, they significantly reduce the need for manual feature engineering.

Neural networks have proven to be extremely effective in a wide variety of complex tasks such as facial recognition, language translation, chatbots etc. and are at the heart of the current AI boom.

Application of Deep Learning to Reinforcement Learning tasks

**Function Approximation**: The most central role of neural networks in Deep RL is indeed as function approximators. In environments with large state and action spaces, it's impractical to store the value of every state-action pair (as in classical RL). Neural networks, with their ability to generalize from limited data, approximate the value function or the policy function, predicting the value of actions or states based on their experience. This makes learning feasible even in complex environments.

**Feature Extraction and Representation Learning**: Neural networks are adept at automatically extracting and learning relevant features from raw input data, which is particularly useful when dealing with high-dimensional inputs like images or sensory data. In traditional RL, feature extraction requires manual engineering and domain knowledge. Neural networks, particularly deep convolutional neural networks, excel at this task in Deep RL, making sense of complex inputs like game frames or sensor arrays.

**Policy Optimization**: In policy-based Deep RL methods, neural networks directly represent the policy of the agent. They map states (or observations) to actions, effectively learning the optimal policy. The network's weights are adjusted to maximize the expected reward over time.

**Handling Partial Observability**: In environments where agents have only partial observability of the state (like in many real-world scenarios), neural networks, especially recurrent neural networks (RNNs) or Long Short-Term Memory networks (LSTMs), can be used to infer the missing information and make decisions based on limited data.

**Multi-Task Learning and Transfer Learning**: Neural networks in Deep RL can be trained on multiple tasks simultaneously or use knowledge gained from one task to improve performance on another (transfer learning). This is particularly useful in complex environments where learning from scratch for every new task would be inefficient.

**Factors affecting algorithm choice**

Due to the broad and complex nature of reinforcement learning problems, selection of an algorithm requires some analysis. We shall now look at some factors that influence the selection of RL algorithm and real world examples that might make them easier to grasp.

**Factors Related to the Environment:**

- Is your environment static or dynamic?
Static: A chess game against a computer. The rules and state of the game don't change unexpectedly.
Dynamic: Stock market prediction. Market conditions can change rapidly and unpredictably.

- Is your environment predictable (deterministic) or random (stochastic)?
Predictable: A puzzle-solving robot in a controlled setting where actions have known outcomes.
Random: Playing poker, where there's uncertainty in cards dealt and opponents' actions.

- Is the environment fully observable or partially observable?
Fully Observable: A video game where the entire game state is visible on the screen.
Partially Observable: Autonomous driving, where the car might not have complete information about all road conditions or other drivers’ intentions.

- Does the problem involve single-agent or multi-agent dynamics?
Single-Agent: A vacuum cleaning robot operating in a home.
Multi-Agent: Drones coordinating in a search and rescue operation.

- Do you have a reliable model of the environment?
Yes: A simulated environment for robotic arm training.
No: Real-world climate modeling, where the complexity and unpredictability of weather systems make it difficult to have a completely accurate model.

- Is exploration in the environment costly or risky?
Costly/Risky: Testing different medical treatment plans where incorrect choices could have severe consequences.
Not Costly/Risky: Trying different layouts in a virtual room designer app.

**Factors Related to the State and Action Space:**

- What is the size of the state and action space?
Small: A tic-tac-toe game.
Large: A complex strategy video game with numerous possible actions and outcomes.

- Is your state space discrete or continuous?
Discrete: Sorting items on a conveyor belt into predefined categories.
Continuous: Adjusting the speed and direction of an autonomous car in real-time.

- Is there a need to generalize the policy across different problems?
Yes: A general-purpose chatbot designed to handle various topics and user queries.
No: A specialized tool for diagnosing a specific type of machinery based on predefined symptoms.

- Is the learning task episodic or continuous?
Episodic: A robot learning to complete a maze - each attempt is a separate episode.
Continuous: An online recommendation system that continuously updates based on user interactions.

- Does the problem require hierarchical decision-making or complex sequential decisions?
Hierarchical/Complex: Managing a supply chain where decisions at one level (like inventory) affect other levels (like distribution).
Simple/Linear: A temperature control system that adjusts based on current temperature readings.

**Factors Related to Rewards:**

- Is the reward structure simple or complex?
Simple: A game where points are scored by hitting targets.
Complex: Customer service interactions where satisfaction is influenced by multiple factors.

- Is the reward structure sparse or dense?
Sparse: Scientific research where breakthroughs (rewards) are rare.
Dense: A training game where players get immediate feedback for each action.

- Is the goal to optimize a short-term or long-term objective?
Short-term: A trading algorithm focused on daily profit.
Long-term: An environmental policy model aiming to reduce emissions over decades.

- How much noise or uncertainty is there in the observations or rewards?
High Noise/Uncertainty: Predicting customer churn in a business with fluctuating customer engagement levels.
Low Noise/Uncertainty: A well-calibrated industrial process where measurements are precise and consistent.

**Practical Considerations:**
- Are there any constraints on computational resources?
High Constraints: A mobile app with limited processing power.
Low Constraints: A data center with powerful servers for processing.

- What are the latency requirements for decision-making?
High Latency Tolerant: A system for analyzing and improving monthly sales strategies.
Low Latency Required: Real-time fraud detection in financial transactions.

- How significant is the need for scalability?
Highly Scalable: A cloud-based service that needs to handle varying loads of user requests.
Limited Scalability Needed: A local traffic control system in a small town.

- Do you have a lot of pretraining data available that can give the model a headstart?
Yes: An image recognition system with access to millions of labeled images.
No: A novel task in a unique environment, like navigating an uncharted underwater terrain, where similar historical data is not available.

Hopefully, this list of factors highlights the complexity of choosing an algorithm and also the vast applicability of reinforcement learning across a number of domains.

I hope this overview of Reinforcement Learning algorithms was useful. In the next post, I will dive into some real world AI systems that use Reinforcement Learning.

## References
1. David Silver's Course on RL - https://www.davidsilver.uk/teaching/
2. Berkeley CS285 - https://rail.eecs.berkeley.edu/deeprlcourse/
3. Stanford CS234 - https://web.stanford.edu/class/cs234/
4. Lillian Weng's blog on Reinforcement Learning  - https://lilianweng.github.io/posts/2018-02-19-rl-overview/
5. Barton & Sutto - http://incompleteideas.net/book/bookdraft2017nov5.pdf
6. OpenAI Spinning Up - https://spinningup.openai.com/en/latest/spinningup/rl_intro.html
7. Volodymyr Mnih, et al. Human-level control through deep reinforcement learning. Nature 518.7540 (2015): 529.
